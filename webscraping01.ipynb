{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "webscraping01.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNa2O5qutLsxMxCJPfQAlVJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hylabrook/ML/blob/master/webscraping01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4BSGvy7Elg7"
      },
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "import time"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "TUnKPKa1FZ6g",
        "outputId": "28341332-7ff0-4b54-d3a8-d27d27081104"
      },
      "source": [
        "web_analysis = {'CareerCross' : 'https://www.careercross.com/en/job-search/result/67335683', \n",
        "                    'DaiJob' : 'https://www.daijob.com/en/jobs/search_result?job_search_form_hidden=1&keywords=Data+Analyst'} \n",
        "\n",
        "titles = []\n",
        "urls = []\n",
        "updates = []\n",
        "locations = []\n",
        "jobs = []\n",
        "salaries = []\n",
        "experiences = []\n",
        "careers = []\n",
        "english_level = []\n",
        "japanese_level=[]\n",
        "educations=[]\n",
        "visas = []\n",
        "skills= []\n",
        "    \n",
        "for w,j in web_analysis.items():\n",
        "    \n",
        "    page = 0\n",
        "    \n",
        "    while True:\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "        r = requests.get(j, params = {\"page\" : page+1})\n",
        "        if 'No jobs were found that matched your search.' in r.text or r.status_code != 200:\n",
        "            break\n",
        "        else:\n",
        "            html = r.content\n",
        "            soup = BeautifulSoup(html, \"lxml\")\n",
        "            print('\\033[1m' + '{0}, page {1}'.format(w,page+1) + '\\033[0m')\n",
        "            \n",
        "\n",
        "            if w == 'CareerCross':\n",
        "                titles_r1 = [t.text.strip().replace('\\n', '') for t in soup.find_all('a', {'class': 'job-details-url'})] #title\n",
        "                titles_r1 = list(map(lambda t: titles.append(t), titles_r1))\n",
        "                full_url = [urljoin(j,r) for r in [l.get('href') for l in soup.find_all('a', {'class': \"btn btn-lg-14 btn-primary\"})]] #url\n",
        "                full_urls = list(map(lambda f: urls.append(f), full_url))\n",
        "                i = 0\n",
        "                for f in full_url: #going to each page\n",
        "                    time.sleep(1) \n",
        "                    r = requests.get(f)\n",
        "                    if r.status_code == 200:\n",
        "                        print('#URL {0}: {1}'.format(i+1, f))\n",
        "                    else:\n",
        "                        print('ERROR {0}: Skipping #URL{1}: {2}'.format(r.status_code, i+1, f))\n",
        "                        i = i+1\n",
        "                        continue\n",
        "                    r_html = r.content\n",
        "                    r_soup = BeautifulSoup(r_html, 'lxml')\n",
        "                    updates.append(r_soup.find_all('span', {'id':\"jsonld-date-posted\"})[0].text.strip()) #date of update\n",
        "                    locations.append(r_soup.find_all('span', {'id': 'jsonld-job-location'})[0].text.strip()) #location\n",
        "                    try:\n",
        "                        job = r_soup.find_all('span', {'id': 'jsonld-employment-type'})[0].text.strip() #type of job\n",
        "                    except IndexError:\n",
        "                        jobs.append(np.nan)\n",
        "                    else:\n",
        "                        jobs.append(job)\n",
        "                    salaries.append(r_soup.find_all('span', {'id': 'jsonld-base-salary'})[0].text.strip()) #salary\n",
        "                    experiences.append(r_soup.find_all('span',  {'id' : \"jsonld-experience-requirements\"})[0].text.strip()) #experience\n",
        "                    careers.append(r_soup.find_all('span',  {'id' : \"jsonld-experience-requirements\"})[1].text.strip()) #career\n",
        "                    english_level.append(r_soup.find_all('span', {'id': \"skill-english-text\"})[0].text.strip()) #english\n",
        "                    japanese_level.append(r_soup.find_all('span', {'id': \"skill-japanese-text\"})[0].text.strip()) #japanese\n",
        "                    educations.append(r_soup.find_all('span', {'id': \"jsonld-education-requirements\"})[0].text.strip()) #education\n",
        "                    visas.append(r_soup.find_all('span' , {'id' : \"qualifications-visa-status\"})[0].text.strip()) #visa\n",
        "                    try:\n",
        "                        skill=r_soup.find_all('span', {'id': 'qualifications-required-skills'})[0].text.strip() #skill description\n",
        "                    except IndexError:\n",
        "                        try:\n",
        "                            skill = [s.find_all('ul') for s in r_soup.find_all('span', {'id':\"jsonld-description\"})][0][2].text.strip()\n",
        "                            if len(skill) < 100:\n",
        "                                skills.append([s.find_all('ul') for s in r_soup.find_all('span', {'id':\"jsonld-description\"})][0][1].text.strip())\n",
        "                            else: \n",
        "                                skills.append(skill)\n",
        "                        except:\n",
        "                            skills.append(r_soup.find_all('span', {'id':\"jsonld-description\"})[0].text.strip())\n",
        "                    else:\n",
        "                        skills.append(skill)\n",
        "                    i=i+1\n",
        "                \n",
        "            elif w == 'DaiJob':\n",
        "                titles_r1 = [t.text.strip() for t in soup.find_all('a', {'id': '_job'})] #title\n",
        "                titles_r1 = list(map(lambda t: titles.append(t), titles_r1))\n",
        "                full_url = [urljoin(j,l) for l in [l.get('href') for l in soup.find_all('a', {'id': '_job'})]] #url\n",
        "                full_urls = list(map(lambda f: urls.append(f), full_url))\n",
        "                i = 0\n",
        "                for f in full_url: #going to each page\n",
        "                    time.sleep(1) \n",
        "                    r = requests.get(f)\n",
        "                    if r.status_code == 200:\n",
        "                        print('#URL {0}: {1}'.format(i+1, f))\n",
        "                    else:\n",
        "                        print('ERROR {0}: Skipping #URL{1}: {2}'.format(r.status_code, i+1, f))\n",
        "                        i = i+1\n",
        "                        continue\n",
        "                    r_html = r.content\n",
        "                    r_soup = BeautifulSoup(r_html, 'lxml')\n",
        "                    updates.append(r_soup.find_all('span', {'class': 'roboto'})[1].text) #date of update\n",
        "                    try:\n",
        "                        loc = r_soup.find_all('td')[3].text.split('\\n')[3].strip() #location\n",
        "                    except IndexError:\n",
        "                        locations.append(r_soup.find_all('td')[4].text.split('\\n')[3].strip())\n",
        "                    else:\n",
        "                        locations.append(loc)\n",
        "                    if 'Job Contract' in r_soup.find_all('tr')[-1].text: #type of job\n",
        "                        jobs.append(r_soup.find_all('td')[-1].text.strip())\n",
        "                    elif 'Job Contract' in r_soup.find_all('tr')[-2].text:\n",
        "                        jobs.append(r_soup.find_all('td')[-2].text.strip())\n",
        "                    else:\n",
        "                        jobs.append(np.nan) \n",
        "                    for s in r_soup.find_all('td'): #salary\n",
        "                        if s.find('a') and 'JPY' in s.text or s.find('a') and 'Depends on experience' in s.text:\n",
        "                            salaries.append(s.text.strip())\n",
        "                    experiences.append(np.nan) #experience\n",
        "                    careers.append(r_soup.find('div', class_ = 'recruit_level').text.strip()) #career\n",
        "                    if 'English Level' in r_soup.find('div', class_=\"jobs_box jobs_box_detail mb25\").text: #english\n",
        "                        for x in range(20):\n",
        "                            if 'English Level' in r_soup.find_all('tr')[x].text:\n",
        "                                english_level.append(r_soup.find_all('td')[x].text.strip())  \n",
        "                                break\n",
        "                    else:\n",
        "                        english_level.append(np.nan)\n",
        "                    if 'Japanese Level' in r_soup.find('div', class_=\"jobs_box jobs_box_detail mb25\").text: #japanese\n",
        "                        for z in range(20):\n",
        "                            if 'Japanese Level' in r_soup.find_all('tr')[z].text:\n",
        "                                japanese_level.append(r_soup.find_all('td')[z].text.strip())  \n",
        "                                break\n",
        "                    else:\n",
        "                        japanese_level.append(np.nan)  \n",
        "                    educations.append(np.nan) #education\n",
        "                    visas.append(np.nan) #visa\n",
        "                    if 'Job Requirements' in r_soup.find('div', class_=\"jobs_box jobs_box_detail mb25\").text: #japanese\n",
        "                        for z in range(20):\n",
        "                            if 'Job Requirements' in r_soup.find_all('tr')[z].text:\n",
        "                                skills.append(r_soup.find_all('td')[z].text.strip().replace(\"\\n\", \" \"))  \n",
        "                                break\n",
        "                    else:\n",
        "                        skills.append(np.nan)  \n",
        "                    i=i+1\n",
        "            page +=1"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mCareerCross, page 1711\u001b[0m\n",
            "\u001b[1mCareerCross, page 1712\u001b[0m\n",
            "\u001b[1mCareerCross, page 1713\u001b[0m\n",
            "\u001b[1mCareerCross, page 1714\u001b[0m\n",
            "\u001b[1mCareerCross, page 1715\u001b[0m\n",
            "\u001b[1mCareerCross, page 1716\u001b[0m\n",
            "\u001b[1mCareerCross, page 1717\u001b[0m\n",
            "\u001b[1mCareerCross, page 1718\u001b[0m\n",
            "\u001b[1mCareerCross, page 1719\u001b[0m\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-389c8254018e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"page\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-9pqNO2GP-8"
      },
      "source": [
        "columns = {'Title': titles, 'URL': urls, 'Update': updates, 'Location': locations, 'Type of job': jobs, 'Salary': salaries, \n",
        "           'Experience needed': experiences, 'Career': careers, 'English Level': english_level, 'Japanese Level': japanese_level,\n",
        "          'Education': educations, 'Visa': visas, 'Skill Description': skills}\n",
        "\n",
        "\n",
        "df_analyst = pd.DataFrame(columns)\n",
        "\n",
        "def make_clickable(val):\n",
        "    return '<a href=\"{0}\">{0}</a>'.format(val)\n",
        "\n",
        "\n",
        "pd.reset_option('display.max_rows', None)\n",
        "pd.reset_option('display.max_columns', None)\n",
        "pd.reset_option('display.width', None)\n",
        "pd.reset_option('display.max_colwidth', -1)\n",
        "\n",
        "df_analyst.style.format({'URL' :make_clickable})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LXmiSQiG3Jl"
      },
      "source": [
        "df_analyst.shape"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}